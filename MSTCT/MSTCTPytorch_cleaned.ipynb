{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MtrOXuGi_yy"
      },
      "source": [
        "# ***MSTCT + Pytorch-I3D***\n",
        "This Colab will contain the code for the 2nd algorithm, to do activity detection for charades. \n",
        "\n",
        "The feature extraction based on piergiaj/pytorch-i3d will also be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghP0ZeYWTu8D"
      },
      "source": [
        "###Set up Google Drive\n",
        "All files from this Colab will be stored in a Google Drivefolder named MSTCT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsPfZ0XmOngH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe7hmghui_Fz"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/MSTCT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STt18039T9Kr"
      },
      "source": [
        "## **1. Setup**\n",
        "Run the cell below to import and install the dependencies needed for the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_CVBCUGNNyn",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip uninstall torch -y\n",
        "!pip uninstall torchvision -y\n",
        "!pip uninstall torchaudio -y\n",
        "!pip install timm==0.4.12\n",
        "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell to create UI templates"
      ],
      "metadata": {
        "id": "3at2_7ZNcBSS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQtXZS6vpH8H",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# imports for UI widgets\n",
        "from IPython.display import HTML, display, Markdown, Video, clear_output\n",
        "from ipywidgets import Layout\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# layout init.\n",
        "btn_layout = widgets.Layout(width='45%', height='40px')\n",
        "btn_sm_layout = widgets.Layout(width='20%', height='40px') \n",
        "label_layout = Layout(width='200px',height='auto')\n",
        "ddl_layout = widgets.Layout(width='45%', height= 'auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NouQ1k8QpL4w"
      },
      "source": [
        "###Check if CUDA is enabled\n",
        "* Check if your device has a CUDA-supported GPU and the cuda version is 11.2. \n",
        "* If the cuda version is mismatched, install the corresponding pytorch tools via pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XervqaiZLEMk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Check if NVIDIA GPU is enabled\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYF8vySgTjY9"
      },
      "source": [
        "### Install Python 3.8 and pip\n",
        "Upgrade python ver in Colab from 3.7 to 3.8 and install pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIpKUfw0ODB6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# install python 3.8\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.8\n",
        "\n",
        "# change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2\n",
        "\n",
        "!sudo apt install python-pip\n",
        "\n",
        "!sudo apt install python3.8-distutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LjlFU8RTO_S"
      },
      "source": [
        "### Install pip library\n",
        "This step updates pip to the version according to python version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPEKH5jgPy1o",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "! python get-pip.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpYF9bjrTd6J"
      },
      "source": [
        "### Check if python 3.8 in installed and pip is updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTN_djw0O8-R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#check python version\n",
        "!python --version\n",
        "#check pip version\n",
        "!pip --version\n",
        "!sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqnWwZmUWlh"
      },
      "source": [
        "###Install the pip dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxcUs7I3PB7R",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install timm==0.4.12\n",
        "!pip install pytorch==1.9.0\n",
        "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install pickle5 # not needed in 3.8\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install h5py\n",
        "!pip install opencv-python\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk6xc-geMqGQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqxaD4Gm9c7f"
      },
      "source": [
        "#### Set up Video Captioning for MSTCT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Umr01nqB3n",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import pandas\n",
        "import cv2\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class CaptioningMST:\n",
        "    def __init__(self, caption_filepath = None):\n",
        "        self.annotation_path = ['./annotations/Charades_v1_train.csv', './annotations/Charades_v1_test.csv']    \n",
        "        self.videopaths = []\n",
        "        self.video_type = ''\n",
        "        self.time = 0\n",
        "        self.output_path = \"./videosCaptionOutput/\"\n",
        "        self.classes = []\n",
        "        self.dataset = {}\n",
        "        self.feature = {}\n",
        "        self.caption_path = caption_filepath\n",
        "        self.accuracydata = {}\n",
        "    \n",
        "    def get_classes(self):\n",
        "        with open('./annotations/Charades_v1_classes.txt', newline='') as classfile:\n",
        "            lines = classfile.readlines()\n",
        "            self.classes.append(lines)\n",
        "\n",
        "    def get_video_names(self):\n",
        "        with open('./annotations/json_data.json', 'r') as f:\n",
        "            self.accuracydata = json.load(f)\n",
        "        self.videopaths = self.accuracydata.keys()\n",
        "\n",
        "    def get_feature(self):\n",
        "        for k,v in self.dataset.items():\n",
        "            listOfFeatures = []\n",
        "            listBeforeConcat = v[9].split(';')\n",
        "            for item in listBeforeConcat:\n",
        "                listOfFeatures.append(item.split(' '))\n",
        "            self.feature[k]=listOfFeatures\n",
        "\n",
        "    def getGroundTruth(self):\n",
        "        self.get_video_names()\n",
        "\n",
        "        with open(self.annotation_path[0], newline='') as csvfile:\n",
        "            spamreader = csv.reader(csvfile)\n",
        "            for row in spamreader:\n",
        "                for item in self.videopaths:\n",
        "                    if(row[0] == item):\n",
        "                        self.dataset[row[0]]=row\n",
        "                        self.video_type = 'test'\n",
        "\n",
        "        if(not self.dataset):\n",
        "            with open(self.annotation_path[1], newline='') as csvfile:\n",
        "                spamreader = csv.reader(csvfile)\n",
        "                for row in spamreader:\n",
        "                    for item in self.videopaths:\n",
        "                        if(row[0] == item):\n",
        "                            self.dataset[row[0]]=row\n",
        "                            self.video_type = 'train'\n",
        "        \n",
        "        if(not self.dataset): return print('Video not found in dataset') \n",
        "\n",
        "        self.get_classes()\n",
        "        self.get_feature()\n",
        "        self.createVideo()\n",
        "\n",
        "    def createVideo(self):\n",
        "        if not os.path.exists('./videosCaptionOutput/'):\n",
        "            os.makedirs('./videosCaptionOutput/')\n",
        "        for video_name in self.videopaths:\n",
        "            cap = cv2.VideoCapture('data/rgbVideos/'+video_name+'.mp4')\n",
        "\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter(self.output_path+video_name+'.mp4', fourcc, 25, (int(cap.get(3)),int(cap.get(4))))\n",
        "            if (cap.isOpened() == False):\n",
        "                print(\"Error opening video stream or file\")\n",
        "                return\n",
        "\n",
        "            while (cap.isOpened()):\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                if not ret:\n",
        "                    print(f'Video annotation for {video_name} is process complete.')\n",
        "                    break\n",
        "                \n",
        "                height, width, channels = frame.shape\n",
        "                cv2.rectangle(frame, (0, int(height * 0.8)), (int(width), int(height*0.98)), (0,0,0), -1)\n",
        "                cv2.putText(frame, \"Ground truth\", (int(width*0.1),int(height*0.85)), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255), 1)\n",
        "                cv2.putText(frame, \"Accuracy\", (int(width*0.75),int(height*0.85)), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255), 1)\n",
        "                cv2.putText(frame, \"Prediction\", (int(width*0.4),int(height*0.85)), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,255,255), 1)\n",
        "                \n",
        "                self.time = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
        "\n",
        "                listofGroundTruths = []\n",
        "                for key, value in self.feature.items():\n",
        "                    if(key == video_name):\n",
        "                        for item in value:\n",
        "                            if(self.time > float(item[1])*1000 and self.time < float(item[2])*1000):\n",
        "                                listofGroundTruths.append(item[0][1:])\n",
        "                \n",
        "                feature_split = np.array_split(listofGroundTruths,2)\n",
        "                cv2.putText(frame, ', '.join(feature_split[0]), (int(width*0.1),int(height*0.90)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)\n",
        "                cv2.putText(frame, ', '.join(feature_split[1]), (int(width*0.1),int(height*0.95)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)\n",
        "\n",
        "                # if want to see the full text\n",
        "                # for index, item in enumerate(listofGroundTruths):\n",
        "                #     cv2.putText(frame, item, (int(width*0.2),int(height*(0.30+(index*0.05)))), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "                #     cv2.putText(frame, self.classes[0][int(item)], (int(width*0.22),int(height*(0.30+(index*0.05)))), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "                    \n",
        "                listOfActivities = []\n",
        "                accuraciesOfActivities = []\n",
        "                for item in self.accuracydata[video_name]['actions']:\n",
        "                    if(self.time > item[1]*1000 and self.time < item[2]*1000):\n",
        "                        listOfActivities.append(item[0])\n",
        "                        accuraciesOfActivities.append(item[3])\n",
        "\n",
        "                activities_split = np.array_split(listOfActivities, 2)\n",
        "                prediction_split = np.array_split(accuraciesOfActivities, 2)\n",
        "                \n",
        "                if(str(feature_split[0])==str(activities_split[0])):\n",
        "                    cv2.putText(frame, ', '.join([str(item) for item in activities_split[0]]), (int(width*0.4),int(height*0.9)), cv2.FONT_HERSHEY_DUPLEX, 0.4, (67,181,75), 1)\n",
        "                else:\n",
        "                    cv2.putText(frame, ', '.join([str(item) for item in activities_split[0]]), (int(width*0.4),int(height*0.9)), cv2.FONT_HERSHEY_DUPLEX, 0.4, (255,255,255), 1)\n",
        "                \n",
        "                if(str(feature_split[1])==str(activities_split[1])):   \n",
        "                    cv2.putText(frame, ', '.join([str(item) for item in activities_split[1]]), (int(width*0.4),int(height*0.95)), cv2.FONT_HERSHEY_DUPLEX, 0.4, (67,181,75), 1)\n",
        "                else:\n",
        "                    cv2.putText(frame, ', '.join([str(item) for item in activities_split[1]]), (int(width*0.4),int(height*0.95)), cv2.FONT_HERSHEY_DUPLEX, 0.4, (255,255,255), 1)\n",
        "                    \n",
        "                cv2.putText(frame, ', '.join([str(item) for item in prediction_split[0]]), (int(width*0.75),int(height*0.90)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)\n",
        "                cv2.putText(frame, ', '.join([str(item) for item in prediction_split[1]]), (int(width*0.75),int(height*0.95)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)\n",
        "                \n",
        "                \n",
        "                out.write(frame)\n",
        "                # cv2.imshow('frame',frame)\n",
        "                key = cv2.waitKey(1)\n",
        "                # define the key to\n",
        "                # close the window\n",
        "                if key == 'q' or key == 27:\n",
        "                    break\n",
        "\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bMEYdrA2YKq"
      },
      "source": [
        "## **2. Feature Extraction**\n",
        "This section performs feature extraction from jpeg folders of videos via [pytorch-i3d](https://github.com/piergiaj/pytorch-i3d)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t11Ot5-3Ud67"
      },
      "source": [
        "### Run the feature extraction file\n",
        "Step 1: Add the videos jpeg folder you want to extract into `/content/drive/MyDrive/MST/pytorch-i3d/Charades_v1_rgb`\n",
        "\n",
        "Step 2: Select `rgb` stream and `rgb_charades` \n",
        "\n",
        "Step 3: Click on `Extact Features` button\n",
        "\n",
        "Step 4: Retrieve the numpy files from `/content/drive/MyDrive/MST/pytorch-i3d/output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNxIdqu_qNIb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd /content/drive/MyDrive/MSTCT/pytorch-i3d\n",
        "# to be replaced with files with /models/ later\n",
        "models_list = ['flow_charades','flow_imagenet','rgb_charades','rgb_imagenet']\n",
        "\n",
        "label_stream = widgets.Label(\"Select Stream:\", layout = label_layout)\n",
        "ddl_stream = widgets.Select(\n",
        "    options=['rgb', 'flow', 'both'],\n",
        "    value='rgb',\n",
        "    layout = ddl_layout)\n",
        "\n",
        "label_models = widgets.Label(\"Select model:\", layout = label_layout)\n",
        "ddl_models = widgets.Dropdown(\n",
        "    options= models_list,\n",
        "    value='rgb_charades',\n",
        "    layout = ddl_layout)\n",
        "\n",
        "btn_extract = widgets.Button(description=\"Extract Features\", layout = btn_layout, button_style='info')\n",
        "\n",
        "extraction_output = widgets.Output()\n",
        "\n",
        "def extract_feature(b):\n",
        "    with extraction_output:\n",
        "        extraction_output.clear_output()\n",
        "        print('run extraction ...')\n",
        "        %cd /content/drive/MyDrive/MSTCT/pytorch-i3d\n",
        "        %run ./extract_features.py -mode $ddl_stream.value -gpu \"0\" -root \"./Charades_v1_rgb\" -save_dir \"./output\" -load_model \"./models/\"$ddl_models.value\n",
        " \n",
        "btn_extract.on_click(extract_feature)\n",
        "\n",
        "# Display\n",
        "feature_box = widgets.VBox([widgets.HBox([label_stream, ddl_stream]), widgets.HBox([label_models, ddl_models]), btn_extract, extraction_output])\n",
        "feature_box  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH5P2Sl6MTue"
      },
      "source": [
        "## MSTCT Repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKCekXenhKdK"
      },
      "source": [
        "### Step 2: Run the training instead of shell script\n",
        "Step 1: Configure the training options using the widgets\n",
        "\n",
        "Step 2: Rename the `pickle output file` to be used later\n",
        "\n",
        "Step 3: Click on `Train Charades` to run training model\n",
        "\n",
        "Step 4: Pickle file is saved in the `/content/drive/MyDrive/MST/MST/save_logit/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJKp6z1OLUBW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%cd /content/drive/MyDrive/MSTCT/MS-TCT\n",
        "\n",
        "# to be replaced with files with /models/ later\n",
        "models_list = ['MS_TCT']\n",
        "\n",
        "# # select training t/f\n",
        "# label_train = widgets.Label(\"Run training\", layout = label_layout)\n",
        "# ddl_train = widgets.Select(\n",
        "#     options=['True','False'],\n",
        "#     value='True',\n",
        "#     layout = ddl_layout)\n",
        "\n",
        "# select model\n",
        "label_models = widgets.Label(\"Select model:\", layout = label_layout)\n",
        "ddl_models_training = widgets.Dropdown(\n",
        "    options= models_list,\n",
        "    value = \"MS_TCT\",\n",
        "    layout = ddl_layout)\n",
        "\n",
        "# select stream\n",
        "label_stream = widgets.Label(\"Select Stream:\", layout = label_layout)\n",
        "ddl_stream_training = widgets.Select(\n",
        "    options=['rgb', 'flow'],\n",
        "    value='rgb',\n",
        "    layout = ddl_layout)\n",
        "\n",
        "\n",
        "# select batch size\n",
        "label_batch_size = widgets.Label(\"batch_size:\", layout = label_layout)\n",
        "batch_size_training = widgets.BoundedIntText(\n",
        "    min=1,\n",
        "    # max=1000, # to be defined and added as necessary\n",
        "    step=1,\n",
        "    value=32,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# select epoch\n",
        "label_epochs = widgets.Label(\"epoch:\", layout = label_layout)\n",
        "epochs_training = widgets.BoundedIntText(\n",
        "    min=1,\n",
        "    # max=1000, # to be defined and added as necessary\n",
        "    step=1,\n",
        "    value=50,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# select unisize\n",
        "label_unisize= widgets.Label(\"unisize:\", layout = label_layout)\n",
        "ddl_unisize = widgets.Select(\n",
        "    options=[True,False],\n",
        "    value=True,\n",
        "    layout = ddl_layout)\n",
        "\n",
        "# select alpha_1\n",
        "label_alpha= widgets.Label(\"alpha_1:\", layout = label_layout)\n",
        "text_alpha = widgets.BoundedIntText(\n",
        "    placeholder='Specify alpha_1 value',\n",
        "    value = 1,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# select beta_1\n",
        "label_beta= widgets.Label(\"beta_1:\", layout = label_layout)\n",
        "text_beta = widgets.BoundedFloatText(\n",
        "    placeholder='Specify beta_1 value',\n",
        "    value = 0.05,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# select comp_info\n",
        "label_comp_info= widgets.Label(\"comp_info:\", layout = label_layout)\n",
        "ddl_comp_info = widgets.Select(\n",
        "    options=[True,False],\n",
        "    value=True,\n",
        "    layout = ddl_layout)\n",
        "\n",
        "# select skip\n",
        "label_skip= widgets.Label(\"skip:\", layout = label_layout)\n",
        "ddl_skip = widgets.Select(\n",
        "    options=[0,1],\n",
        "    value=0,\n",
        "    layout = ddl_layout)\n",
        "\n",
        "# select lr\n",
        "label_lr= widgets.Label(\"lr:\", layout = label_layout)\n",
        "lr_text_lr = widgets.BoundedFloatText(\n",
        "    placeholder='Specify lr value',\n",
        "    value=0.0001,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "label_save_file = widgets.Label(\"file name (output):\", layout = label_layout)\n",
        "text_save_file = widgets.Text(\n",
        "    placeholder='Specify file name',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "training_Confirm_Button  = widgets.Button(description=\"Train Charades\", \n",
        "                                          layout = btn_layout, \n",
        "                                          button_style='info')\n",
        "\n",
        "mstct_training_output = widgets.Output()\n",
        "\n",
        "\n",
        "def train_features(b):\n",
        "    with mstct_training_output:\n",
        "        mstct_training_output.clear_output()\n",
        "        %cd /content/drive/MyDrive/MSTCT/MS-TCT\n",
        "        if text_save_file.value == '':\n",
        "          print('Make a unique file name')\n",
        "          return\n",
        "        print('run training ...')\n",
        "        %run train.py -dataset \"charades\" -gpu 0 -mode $ddl_stream_training.value -model $ddl_models_training.value -train True -num_clips 256 -skip $ddl_skip.value -lr $lr_text_lr.value -comp_info $ddl_comp_info.value -epoch $epochs_training.value -unisize $ddl_unisize.value -alpha_l $text_beta.value -beta_l $text_beta.value -batch_size $batch_size_training.value -save_file_name $text_save_file.value\n",
        "        CaptioningMST().getGroundTruth()\n",
        "        \n",
        "training_Confirm_Button.on_click(train_features)\n",
        "\n",
        "\n",
        "# Display\n",
        "feature_box = widgets.VBox([widgets.HBox([label_models, ddl_models_training]),\n",
        "                            widgets.HBox([label_stream, ddl_stream_training]), \n",
        "                            widgets.HBox([label_batch_size, batch_size_training]), \n",
        "                            widgets.HBox([label_epochs, epochs_training]), \n",
        "                            widgets.HBox([label_unisize, ddl_unisize]), \n",
        "                            widgets.HBox([label_alpha, text_alpha]), \n",
        "                            widgets.HBox([label_beta, text_beta]), \n",
        "                            widgets.HBox([label_comp_info, ddl_comp_info]), \n",
        "                            widgets.HBox([label_skip, ddl_skip]), \n",
        "                            widgets.HBox([label_lr, lr_text_lr]), \n",
        "                            widgets.HBox([label_save_file, text_save_file]), \n",
        "                            training_Confirm_Button,\n",
        "                            mstct_training_output])\n",
        "feature_box\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAc-Y-6zfkR_"
      },
      "source": [
        "Testing for MSTCT\n",
        "\n",
        "Step 1: Select Pickle file to test using the drop down selection list referencing the `/content/drive/MyDrive/MST/MST/save_logit/` directory\n",
        "\n",
        "Step 2 : Run testing and see the probability of each video\n",
        "\n",
        "Step 3: Put videos into `/content/drive/MyDrive/MST/MST/data/rgbVideos`\n",
        "\n",
        "Step 4 : Accuracies of each batch of video will be stored into `/content/drive/MyDrive/MST/MST/annotations/json_data.json`\n",
        "\n",
        "Step 5: Wait for videos to generate to `/content/drive/MyDrive/MST/MST/videosCaptionOutput`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWptnE9-kxyL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "\n",
        "def populateList(fileDirectory, fileType):\n",
        "    folder_files = os.listdir(\"/content/drive/MyDrive/MSTCT/MS-TCT/save_logit\") #You can also use full path.\n",
        "    print(\"This Folder contains {len_folder} file(s).\".format(len_folder=len(folder_files)))\n",
        "    fileList = []\n",
        "    for file in folder_files:\n",
        "        if file.endswith(fileType):\n",
        "            fileList.append(file)\n",
        "    return fileList\n",
        "\n",
        "pickleList = populateList('MS-TCT/save_logit', \".pkl\")\n",
        "        \n",
        "## dropdown list from the ./save_logit/ to allow user to select the output files\n",
        "label_pickleList = widgets.Label(\"Select Pickle file:\", layout = label_layout)\n",
        "ddl_pickleList = widgets.Dropdown(\n",
        "    options= pickleList,\n",
        "    value= pickleList[0],\n",
        "    layout = ddl_layout)\n",
        "\n",
        "button_execute = widgets.Button(description=\"Generate Video\", \n",
        "                                          layout = btn_layout, \n",
        "                                          button_style='info')\n",
        "\n",
        "\n",
        "generate_video_output = widgets.Output()\n",
        "\n",
        "# To add onclick function here\n",
        "def execute(b):\n",
        "    with generate_video_output:\n",
        "      generate_video_output.clear_output()\n",
        "      %cd /content/drive/MyDrive/MSTCT/MS-TCT\n",
        "      %run Evaluation.py -pkl_path ./save_logit/$ddl_pickleList.value\n",
        "        \n",
        "button_execute.on_click(execute)\n",
        "\n",
        "# Display\n",
        "feature_box = widgets.VBox([widgets.HBox([label_pickleList, ddl_pickleList]),\n",
        "                            button_execute, generate_video_output])\n",
        "feature_box"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}